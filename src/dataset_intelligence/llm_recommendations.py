"""
LLM-based semantic graph system for intelligent dataset discovery.

This module provides a simplified approach to semantic graph recommendations using
LLM reasoning instead of complex pattern matching and scoring algorithms.
Designed for A/B testing against the existing multi-strategy system.
"""

import json
import sys
import asyncio
import asyncpg
from typing import List, Dict, Optional, Any
from dataclasses import dataclass
from datetime import datetime

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False


@dataclass
class LLMDatasetRecommendation:
    """
    A dataset recommendation generated by LLM reasoning.
    """
    dataset_id: str
    name: str
    dataset_type: str
    business_category: str
    technical_category: str
    relevance_score: float  # 0.0-1.0 (converted from LLM's 1-10 scale)
    explanation: str
    matching_factors: List[str]
    key_fields: List[str]
    confidence: float  # LLM's confidence in the recommendation


class LLMDatasetRecommendationEngine:
    """
    LLM-powered dataset recommendation engine.
    Replaces complex pattern matching with natural language reasoning.
    """
    
    def __init__(self, db_config: Dict[str, str]):
        """
        Initialize the LLM recommendation engine.
        
        Args:
            db_config: Database connection configuration
        """
        self.db_config = db_config
        self.connection_pool = None
        
    async def initialize(self):
        """Initialize database connection pool."""
        try:
            self.connection_pool = await asyncpg.create_pool(
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database'],
                user=self.db_config['user'],
                password=self.db_config['password'],
                min_size=1,
                max_size=5,
                command_timeout=30
            )
            print("[SEMANTIC_GRAPH] Database connection pool initialized", file=sys.stderr)
        except Exception as e:
            print(f"[SEMANTIC_GRAPH] Failed to initialize database connection: {e}", file=sys.stderr)
            raise
    
    async def close(self):
        """Close database connection pool."""
        if self.connection_pool:
            await self.connection_pool.close()
    
    async def get_dataset_candidates(self, query: str, max_candidates: int = 30) -> List[Dict[str, Any]]:
        """
        Get dataset candidates from the database with improved diversity.
        Uses smart sampling to ensure domain coverage instead of simple ordering.
        
        Args:
            query: User's natural language query  
            max_candidates: Maximum number of candidates to retrieve
            
        Returns:
            List of dataset records with full metadata
        """
        if not self.connection_pool:
            await self.initialize()
        
        try:
            async with self.connection_pool.acquire() as conn:
                # Smart candidate selection strategy:
                # 1. Get domain-specific datasets first (if query suggests specific domain)
                # 2. Get diverse sampling across categories
                # 3. Fill remaining slots with general datasets
                
                query_lower = query.lower()
                candidates = []
                
                # Step 1: Domain-specific prioritization with explicit dataset detection
                domain_hints = {
                    'database': ['database', 'sql', 'query', 'db'],
                    'kubernetes': ['kubernetes', 'k8s', 'pod', 'container', 'cluster'],
                    'logs': ['log', 'logs', 'logging', 'event', 'message'],
                    'traces': ['trace', 'tracing', 'span', 'distributed'],
                    'metrics': ['metric', 'metrics', 'monitoring', 'measurement'],
                    'opentelemetry': ['opentelemetry', 'otel', 'telemetry']
                }
                
                # Explicit dataset name patterns for priority boosting
                explicit_patterns = {
                    'OpenTelemetry/Span': ['span data', 'spans', 'trace data', 'otel span', 'opentelemetry span'],
                    'Kubernetes Explorer/Kubernetes Logs': ['k8s logs', 'kubernetes logs', 'kube logs'],
                    'ServiceExplorer/Database Call': ['database calls', 'db calls', 'database call'],
                    'Kubernetes Explorer/Prometheus Metrics': ['prometheus metrics', 'prom metrics'],
                    'OpenTelemetry/Trace': ['trace data', 'traces', 'distributed trace'],
                    'OpenTelemetry/Span Event': ['span events', 'span event data']
                }
                
                # Check for explicit dataset mentions
                explicit_datasets = []
                for dataset_name, patterns in explicit_patterns.items():
                    if any(pattern in query_lower for pattern in patterns):
                        explicit_datasets.append(dataset_name)
                        print(f"[SEMANTIC_GRAPH] Detected explicit dataset request: {dataset_name}", file=sys.stderr)
                
                # Step 1a: Get explicitly requested datasets FIRST (highest priority)
                if explicit_datasets:
                    for dataset_name in explicit_datasets:
                        exact_sql = """
                        SELECT 
                            dataset_id, name, description, business_category, technical_category,
                            key_fields, interfaces, dataset_type, typical_usage, schema_info,
                            last_analyzed, created_at
                        FROM dataset_intelligence 
                        WHERE excluded = false AND name = $1
                        LIMIT 1
                        """
                        exact_rows = await conn.fetch(exact_sql, dataset_name)
                        if exact_rows:
                            candidates.extend(exact_rows)
                            print(f"[SEMANTIC_GRAPH] Found explicit dataset: {dataset_name}", file=sys.stderr)
                
                # Step 1b: For span/trace queries, ensure OpenTelemetry/Span is included
                if any(term in query_lower for term in ['span', 'trace', 'latency', 'performance', 'distributed']):
                    span_sql = """
                    SELECT 
                        dataset_id, name, description, business_category, technical_category,
                        key_fields, interfaces, dataset_type, typical_usage, schema_info,
                        last_analyzed, created_at
                    FROM dataset_intelligence 
                    WHERE excluded = false AND name = 'OpenTelemetry/Span'
                    LIMIT 1
                    """
                    span_rows = await conn.fetch(span_sql)
                    if span_rows:
                        # Avoid duplicates
                        existing_ids = {row['dataset_id'] for row in candidates}
                        for row in span_rows:
                            if row['dataset_id'] not in existing_ids:
                                candidates.append(row)
                                print("[SEMANTIC_GRAPH] Ensured OpenTelemetry/Span is included for trace query", file=sys.stderr)
                
                # Step 1c: Domain-specific prioritization
                prioritized_domains = []
                for domain, keywords in domain_hints.items():
                    if any(keyword in query_lower for keyword in keywords):
                        prioritized_domains.append(domain)
                
                # Get additional domain-specific datasets (up to 12 more candidates)
                remaining_domain_slots = 12 - len(candidates)
                if prioritized_domains and remaining_domain_slots > 0:
                    domain_conditions = []
                    for domain in prioritized_domains:
                        domain_conditions.append(f"LOWER(name) LIKE '%{domain}%'")
                        domain_conditions.append(f"LOWER(description) LIKE '%{domain}%'")
                        domain_conditions.append(f"LOWER(business_category) LIKE '%{domain}%'")
                        domain_conditions.append(f"LOWER(technical_category) LIKE '%{domain}%'")
                    
                    domain_sql = f"""
                    SELECT 
                        dataset_id, name, description, business_category, technical_category,
                        key_fields, interfaces, dataset_type, typical_usage, schema_info,
                        last_analyzed, created_at
                    FROM dataset_intelligence 
                    WHERE excluded = false 
                        AND ({' OR '.join(domain_conditions)})
                    ORDER BY 
                        CASE 
                            WHEN LOWER(name) LIKE '%{prioritized_domains[0]}%' THEN 1
                            WHEN LOWER(description) LIKE '%{prioritized_domains[0]}%' THEN 2  
                            ELSE 3
                        END,
                        last_analyzed DESC NULLS LAST
                    LIMIT {remaining_domain_slots}
                    """
                    
                    domain_rows = await conn.fetch(domain_sql)
                    
                    # Avoid duplicates
                    existing_ids = {row['dataset_id'] for row in candidates}
                    for row in domain_rows:
                        if row['dataset_id'] not in existing_ids:
                            candidates.append(row)
                            existing_ids.add(row['dataset_id'])
                    
                    print(f"[SEMANTIC_GRAPH] Found {len([r for r in domain_rows if r['dataset_id'] not in existing_ids])} additional domain-specific candidates for: {prioritized_domains}", file=sys.stderr)
                
                # Step 2: Get diverse sampling across different categories
                remaining_slots = max_candidates - len(candidates)
                if remaining_slots > 0:
                    # Get a mix from different business categories to ensure diversity
                    diversity_sql = """
                    SELECT 
                        dataset_id, name, description, business_category, technical_category,
                        key_fields, interfaces, dataset_type, typical_usage, schema_info,
                        last_analyzed, created_at,
                        CASE business_category
                            WHEN 'Application' THEN 1
                            WHEN 'Infrastructure' THEN 2  
                            WHEN 'Monitoring' THEN 3
                            WHEN 'Database' THEN 4
                            ELSE 5
                        END as category_priority
                    FROM dataset_intelligence 
                    WHERE excluded = false
                    ORDER BY 
                        category_priority,
                        RANDOM()  -- Add randomness for diversity
                    LIMIT $1
                    """
                    
                    diversity_rows = await conn.fetch(diversity_sql, remaining_slots)
                    
                    # Avoid duplicates from domain-specific selection
                    existing_ids = {row['dataset_id'] for row in candidates}
                    for row in diversity_rows:
                        if row['dataset_id'] not in existing_ids:
                            candidates.append(row)
                            existing_ids.add(row['dataset_id'])
                
                # Convert to our standard format
                formatted_candidates = []
                for row in candidates:
                    candidate = {
                        'dataset_id': row['dataset_id'],
                        'name': row['name'],
                        'description': row['description'] or 'No description available',
                        'business_category': row['business_category'] or 'Unknown',
                        'technical_category': row['technical_category'] or 'Unknown',
                        'key_fields': list(row['key_fields']) if row['key_fields'] else [],
                        'interfaces': row['interfaces'] if row['interfaces'] else {},
                        'dataset_type': row['dataset_type'] or 'Unknown', 
                        'typical_usage': row['typical_usage'] or 'General analysis',
                        'schema_info': row['schema_info'] if row['schema_info'] else {},
                        'last_analyzed': row['last_analyzed'],
                        'created_at': row['created_at']
                    }
                    formatted_candidates.append(candidate)
                
                print(f"[SEMANTIC_GRAPH] Retrieved {len(formatted_candidates)} dataset candidates with smart sampling", file=sys.stderr)
                return formatted_candidates
                
        except Exception as e:
            print(f"[SEMANTIC_GRAPH] Error retrieving candidates: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
            return []
    
    def prepare_llm_context(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Prepare dataset candidates for LLM processing.
        Simplifies and structures data for optimal LLM reasoning.
        
        Args:
            candidates: Raw dataset records
            
        Returns:
            Simplified candidate data for LLM
        """
        llm_candidates = []
        
        for candidate in candidates:
            # Extract interface types from JSONB
            interface_types = []
            if isinstance(candidate['interfaces'], list):
                for interface_obj in candidate['interfaces']:
                    if isinstance(interface_obj, dict) and 'path' in interface_obj:
                        interface_types.append(interface_obj['path'])
            elif isinstance(candidate['interfaces'], dict):
                # Handle dict format - extract values or keys
                interface_types = list(candidate['interfaces'].keys()) or list(candidate['interfaces'].values())
            
            # Extract sample schema fields
            sample_fields = []
            if isinstance(candidate['schema_info'], dict):
                columns = candidate['schema_info'].get('columns', [])
                if isinstance(columns, list):
                    sample_fields = [col.get('name', '') for col in columns[:8] if isinstance(col, dict)]
            
            llm_candidate = {
                'id': candidate['dataset_id'],
                'name': candidate['name'],
                'description': candidate['description'],
                'business_domain': candidate['business_category'],
                'data_type': candidate['technical_category'],
                'dataset_type': candidate['dataset_type'],
                'key_fields': candidate['key_fields'][:10],  # Top 10 key fields
                'interface_types': interface_types,
                'typical_usage': candidate['typical_usage'],
                'sample_schema_fields': sample_fields
            }
            
            llm_candidates.append(llm_candidate)
        
        return llm_candidates
    
    async def call_llm_for_ranking(self, query: str, candidates: List[Dict[str, Any]]) -> Optional[List[Dict[str, Any]]]:
        """
        Call LLM to rank and explain dataset relevance.
        
        Args:
            query: User's natural language query
            candidates: Prepared candidate data
            
        Returns:
            LLM response with rankings and explanations
        """
        if not OPENAI_AVAILABLE:
            print("[SEMANTIC_GRAPH] OpenAI not available for LLM ranking", file=sys.stderr)
            return None
        
        try:
            import os
            api_key = os.getenv("OPENAI_API_KEY") or os.getenv("SMART_TOOLS_API_KEY")
            if not api_key:
                print("[SEMANTIC_GRAPH] No OpenAI API key available", file=sys.stderr)
                return None
            
            client = openai.OpenAI(api_key=api_key)
            
            # Enhanced prompt with domain expertise and better ranking logic
            # Detect if user has explicit dataset requests
            query_lower = query.lower()
            explicit_requests = []
            
            explicit_patterns = {
                'OpenTelemetry/Span': ['span data', 'spans', 'trace data', 'otel span', 'opentelemetry span'],
                'Kubernetes Explorer/Kubernetes Logs': ['k8s logs', 'kubernetes logs', 'kube logs'],
                'ServiceExplorer/Database Call': ['database calls', 'db calls', 'database call'],
                'Kubernetes Explorer/Prometheus Metrics': ['prometheus metrics', 'prom metrics'],
                'OpenTelemetry/Trace': ['trace data', 'traces', 'distributed trace'],
                'OpenTelemetry/Span Event': ['span events', 'span event data']
            }
            
            for dataset_name, patterns in explicit_patterns.items():
                if any(pattern in query_lower for pattern in patterns):
                    explicit_requests.append(dataset_name)
            
            explicit_instruction = ""
            if explicit_requests:
                explicit_instruction = f"\nðŸŽ¯ EXPLICIT DATASET REQUEST DETECTED: The user specifically mentioned {', '.join(explicit_requests)}. These datasets MUST be scored 9.8+ and ranked at the top.\n"
            
            prompt = f"""You are a senior observability engineer with deep expertise in distributed systems, helping analysts find the optimal datasets for their investigation.

USER QUERY: "{query}"
{explicit_instruction}
AVAILABLE DATASETS:
{json.dumps(candidates, indent=2)}

ANALYSIS FRAMEWORK:
1. **Intent Classification**: What is the user trying to achieve?
   - Performance Analysis: latency, response time, slow queries
   - Error Investigation: failures, exceptions, error rates  
   - Infrastructure Monitoring: CPU, memory, pods, nodes
   - Service Debugging: traces, spans, service maps
   - Log Analysis: events, messages, error patterns

2. **Domain Matching Priority**:
   - EXACT MATCH: Query terms directly match dataset interface/purpose (logsâ†’log datasets, databaseâ†’database datasets)
   - STRONG MATCH: Dataset type aligns with investigation needs (traces for performance, metrics for monitoring)
   - CONTEXTUAL MATCH: Related but indirect (service metrics for performance analysis)

3. **Dataset Quality Assessment**:
   - Primary datasets (exact data needed) score 9-10
   - Supporting datasets (contextual data) score 7-8  
   - Related datasets (might be useful) score 5-6

CRITICAL RULES:
- If user mentions "logs/logging", prioritize datasets with 'log' interface types
- If user mentions "database/SQL", prioritize datasets with 'database' in name/description
- If user mentions "kubernetes/k8s/pods", prioritize Kubernetes Explorer datasets
- If user mentions "performance/latency/traces", prioritize span/trace datasets
- If user mentions "errors/failures", prioritize both logs AND traces (errors appear in both)
- For span/trace queries: OpenTelemetry/Span is THE primary dataset - always rank it #1 for trace analysis (score 9.9)
- If user explicitly mentions dataset names (e.g., "k8s logs", "kubernetes logs", "span data", "database calls"), prioritize exact matches
- SPECIAL RULE: For any query mentioning "span", "trace", "latency", "performance" â†’ OpenTelemetry/Span gets automatic 9.9 score

EXPLICIT DATASET MATCHING:
- "k8s logs" / "kubernetes logs" â†’ Kubernetes Explorer/Kubernetes Logs (score 9.8+)
- "span data" / "spans" / "trace data" â†’ OpenTelemetry/Span (score 9.9+)
- "database calls" / "db calls" â†’ ServiceExplorer/Database Call (score 9.8+)
- "prometheus metrics" â†’ Kubernetes Explorer/Prometheus Metrics (score 9.8+)
- Any explicit dataset family mention gets priority boost

RESPONSE FORMAT: JSON array of TOP 10 datasets, ordered by relevance:

```json
[
  {{
    "dataset_id": "exact_id_from_input", 
    "relevance_score": 9.2,
    "explanation": "This dataset contains exactly the [specific data type] needed for [specific analysis]. It captures [key fields] which directly addresses your query about [query aspect].",
    "matching_factors": ["exact_interface_match", "contains_required_fields", "optimal_for_use_case"],
    "confidence": 0.95,
    "dataset_category": "primary|supporting|contextual"
  }}
]
```

SCORING CALIBRATION:
- 9.5-10.0: Perfect dataset - exactly what the query needs (e.g., "database performance" â†’ Database Call datasets)
- 8.5-9.4: Excellent dataset - very high relevance with direct value  
- 7.5-8.4: Good dataset - relevant and useful for the investigation
- 6.5-7.4: Decent dataset - provides supporting information
- 5.0-6.4: Marginal dataset - might be useful but not primary
- <5.0: Not relevant enough to include

Be specific about WHY each dataset is relevant. Mention exact field names, data types, or use cases that match the query."""

            response = client.chat.completions.create(
                model="gpt-4o-mini",  # Use faster model for ranking
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,  # Low temperature for consistent ranking
                max_tokens=2000
            )
            
            response_text = response.choices[0].message.content.strip()
            
            # Extract JSON from response
            try:
                # Look for JSON block in response
                if "```json" in response_text:
                    json_start = response_text.find("```json") + 7
                    json_end = response_text.find("```", json_start)
                    json_text = response_text[json_start:json_end].strip()
                elif response_text.startswith('['):
                    json_text = response_text
                else:
                    # Try to find JSON array in response
                    import re
                    json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
                    if json_match:
                        json_text = json_match.group(0)
                    else:
                        raise ValueError("No JSON found in LLM response")
                
                llm_rankings = json.loads(json_text)
                print(f"[SEMANTIC_GRAPH] LLM ranked {len(llm_rankings)} datasets", file=sys.stderr)
                return llm_rankings
                
            except (json.JSONDecodeError, ValueError) as e:
                print(f"[SEMANTIC_GRAPH] Error parsing LLM JSON response: {e}", file=sys.stderr)
                print(f"[SEMANTIC_GRAPH] Raw response: {response_text[:500]}...", file=sys.stderr)
                return None
            
        except Exception as e:
            print(f"[SEMANTIC_GRAPH] Error calling LLM: {e}", file=sys.stderr)
            import traceback
            traceback.print_exc(file=sys.stderr)
            return None
    
    def convert_llm_to_recommendations(
        self, 
        llm_rankings: List[Dict[str, Any]], 
        candidates: List[Dict[str, Any]]
    ) -> List[LLMDatasetRecommendation]:
        """
        Convert LLM rankings to recommendation objects with enhanced processing.
        
        Args:
            llm_rankings: LLM ranking results
            candidates: Original candidate data
            
        Returns:
            List of recommendation objects with quality improvements
        """
        recommendations = []
        
        # Create lookup for original candidate data
        candidate_lookup = {c['dataset_id']: c for c in candidates}
        
        for ranking in llm_rankings:
            try:
                dataset_id = ranking['dataset_id']
                original_data = candidate_lookup.get(dataset_id)
                
                if not original_data:
                    print(f"[SEMANTIC_GRAPH] Warning: LLM referenced unknown dataset {dataset_id}", file=sys.stderr)
                    continue
                
                # Convert 1-10 scale to 0.0-1.0
                relevance_score = min(max(float(ranking['relevance_score']) / 10.0, 0.0), 1.0)
                confidence = min(max(float(ranking.get('confidence', 0.8)), 0.0), 1.0)
                
                # Enhanced explanation processing
                explanation = ranking.get('explanation', 'Relevant for your query')
                
                # Clean up and enhance matching factors
                matching_factors = ranking.get('matching_factors', [])
                if isinstance(matching_factors, str):
                    matching_factors = [matching_factors]
                
                # Add dataset category context if provided by LLM
                dataset_category = ranking.get('dataset_category', 'general')
                if dataset_category and dataset_category != 'general':
                    matching_factors.append(f"{dataset_category}_dataset")
                
                recommendation = LLMDatasetRecommendation(
                    dataset_id=dataset_id,
                    name=original_data['name'],
                    dataset_type=original_data['dataset_type'],
                    business_category=original_data['business_category'],
                    technical_category=original_data['technical_category'],
                    relevance_score=relevance_score,
                    explanation=explanation,
                    matching_factors=matching_factors,
                    key_fields=original_data['key_fields'],
                    confidence=confidence
                )
                
                recommendations.append(recommendation)
                
            except (KeyError, ValueError, TypeError) as e:
                print(f"[SEMANTIC_GRAPH] Error processing LLM ranking: {e}, ranking: {ranking}", file=sys.stderr)
                continue
        
        # Post-processing: Ensure quality and diversity
        return self._enhance_recommendation_quality(recommendations)
    
    def _enhance_recommendation_quality(self, recommendations: List[LLMDatasetRecommendation]) -> List[LLMDatasetRecommendation]:
        """
        Post-process recommendations to improve quality and diversity.
        
        Args:
            recommendations: Initial recommendations from LLM
            
        Returns:
            Enhanced recommendations with better ordering and diversity
        """
        if not recommendations:
            return recommendations
        
        # Sort by relevance score (highest first)
        recommendations.sort(key=lambda x: x.relevance_score, reverse=True)
        
        # Ensure diversity: avoid too many datasets from same family
        enhanced_recs = []
        dataset_families = {}
        
        for rec in recommendations:
            # Extract dataset family (e.g., "OpenTelemetry", "ServiceExplorer", "Kubernetes Explorer")
            name_parts = rec.name.split('/')
            family = name_parts[0] if '/' in rec.name else rec.business_category
            
            # Limit datasets per family (except for very high scores)
            family_count = dataset_families.get(family, 0)
            
            # Allow up to 3 per family for high-scoring datasets, 2 for medium
            max_per_family = 3 if rec.relevance_score > 0.85 else 2
            
            if family_count < max_per_family or rec.relevance_score > 0.9:
                enhanced_recs.append(rec)
                dataset_families[family] = family_count + 1
            elif len(enhanced_recs) < 3:  # Always include top 3 regardless
                enhanced_recs.append(rec)
                dataset_families[family] = family_count + 1
        
        # Quality enhancement: Boost explanations for top results
        for i, rec in enumerate(enhanced_recs[:5]):
            if rec.relevance_score > 0.9 and not any(word in rec.explanation.lower() 
                                                   for word in ['perfect', 'ideal', 'exactly']):
                # Enhance high-scoring explanations
                if i == 0:
                    rec.explanation = f"ðŸŽ¯ PRIMARY DATASET: {rec.explanation}"
                elif rec.relevance_score > 0.85:
                    rec.explanation = f"â­ HIGH-VALUE: {rec.explanation}"
        
        print(f"[SEMANTIC_GRAPH] Enhanced recommendations: {len(enhanced_recs)} total, families: {list(dataset_families.keys())}", file=sys.stderr)
        return enhanced_recs
    
    async def recommend_datasets(
        self, 
        query: str, 
        limit: int = 10,
        min_score: float = 0.5,
        categories: Optional[List[str]] = None
    ) -> List[LLMDatasetRecommendation]:
        """
        Main method to recommend datasets using LLM reasoning.
        
        Args:
            query: Natural language query
            limit: Maximum number of recommendations
            min_score: Minimum relevance score threshold (0.0-1.0)
            categories: Optional category filter (not implemented yet)
            
        Returns:
            List of dataset recommendations ordered by relevance
        """
        print(f"[SEMANTIC_GRAPH] Processing query: {query[:100]}...", file=sys.stderr)
        
        # Step 1: Get dataset candidates from database
        max_candidates = min(10, limit * 2)  # Get 2x limit for good selection
        candidates = await self.get_dataset_candidates(query, max_candidates)
        
        if not candidates:
            print("[SEMANTIC_GRAPH] No dataset candidates found", file=sys.stderr)
            return []
        
        # Step 2: Prepare data for LLM
        llm_candidates = self.prepare_llm_context(candidates)
        
        # Step 3: Get LLM rankings
        llm_rankings = await self.call_llm_for_ranking(query, llm_candidates)
        
        if not llm_rankings:
            print("[SEMANTIC_GRAPH] No LLM rankings received", file=sys.stderr)
            return []
        
        # Step 4: Convert to recommendation objects
        recommendations = self.convert_llm_to_recommendations(llm_rankings, candidates)
        
        # Step 5: Apply filters and limits
        filtered_recommendations = [
            rec for rec in recommendations 
            if rec.relevance_score >= min_score
        ]
        
        # Already sorted by LLM, just apply limit
        final_recommendations = filtered_recommendations[:limit]
        
        print(f"[SEMANTIC_GRAPH] Returning {len(final_recommendations)} recommendations", file=sys.stderr)
        return final_recommendations


# Global instance
_llm_recommendation_engine = None


async def get_llm_recommendation_engine() -> LLMDatasetRecommendationEngine:
    """Get or create the global LLM recommendation engine instance."""
    global _llm_recommendation_engine
    
    if _llm_recommendation_engine is None:
        import os
        db_config = {
            'host': os.getenv('POSTGRES_HOST', 'localhost'),
            'port': int(os.getenv('POSTGRES_PORT', 5432)),
            'database': os.getenv('POSTGRES_DB', 'semantic_graph'),
            'user': os.getenv('POSTGRES_USER', 'semantic_graph'),
            'password': os.getenv('POSTGRES_PASSWORD', os.getenv('SEMANTIC_GRAPH_PASSWORD', ''))
        }
        
        _llm_recommendation_engine = LLMDatasetRecommendationEngine(db_config)
        await _llm_recommendation_engine.initialize()
    
    return _llm_recommendation_engine


async def query_datasets_llm(
    query: str,
    limit: int = 10,
    min_score: float = 0.5,
    categories: Optional[List[str]] = None
) -> List[LLMDatasetRecommendation]:
    """
    Main function to recommend datasets using LLM reasoning.
    
    This is the LLM-based alternative to the existing complex scoring system.
    Designed for A/B testing against the current multi-strategy approach.
    
    Args:
        query: Natural language query describing what data you're looking for
        limit: Maximum number of recommendations to return
        min_score: Minimum relevance score threshold (0.0-1.0)
        categories: Optional list of business categories to filter by
        
    Returns:
        List of LLMDatasetRecommendation objects ordered by relevance
        
    Examples:
        recommendations = await query_datasets_llm(
            "Show me service error rates and performance issues"
        )
        
        recommendations = await query_datasets_llm(
            "Find CPU and memory usage for containers",
            categories=["Infrastructure"]
        )
    """
    engine = await get_llm_recommendation_engine()
    return await engine.recommend_datasets(query, limit, min_score, categories)


if __name__ == "__main__":
    # Test the LLM recommendation system
    async def test_llm_recommendations():
        test_queries = [
            "Show me service error rates by application",
            "Find slow database queries and performance issues",
            "Analyze container CPU and memory usage patterns",
            "Debug authentication failures in microservices",
            "Monitor Kubernetes pod restarts and failures"
        ]
        
        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"Testing: {query}")
            try:
                results = await query_datasets_llm(query, limit=5)
                print(f"Found {len(results)} recommendations")
                for i, rec in enumerate(results, 1):
                    print(f"\n{i}. {rec.name} (score: {rec.relevance_score:.2f})")
                    print(f"   Explanation: {rec.explanation}")
                    print(f"   Factors: {', '.join(rec.matching_factors)}")
            except Exception as e:
                print(f"Error: {e}")
                import traceback
                traceback.print_exc()
    
    asyncio.run(test_llm_recommendations())